# Web Scraping and Data Cleansing Tool
This project is a Python-based web scraping and data cleansing tool designed to extract and clean data from licensed liquor premises and suppliers in Hong Kong.

---
## ðŸ“Œ Code Installation  
Before running the script, install the required dependencies:
```
pip install requests beautifulsoup4 selenium pandas openpyxl
```

## ðŸš€ Features
âœ… Automated data extraction using Python, BeautifulSoup, and Selenium. <br/>
âœ… Data cleaning and preprocessing using Pandas. <br/>
âœ… Handles over 50+ cuisine types & multiple district IDs for comprehensive data collection.<br/>
âœ… Multi-threaded execution to optimize web scraping performance.<br/>
âœ… Exports cleaned data to Excel (.xlsx) format.
## ðŸ“Œ Main Section
### ðŸ“ Web Scraping Code
```
import requests
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import WebDriverException, TimeoutException, NoSuchElementException
import time
import pandas as pd
import re
from urllib.parse import quote
import concurrent.futures
import threading
```
## ðŸš— Initializing WebDriver
```
# List of cuisines and district IDs
cuisines = [
    "ç²µèœ-å»£æ±", "æ½®å·žèœ", "å®¢å®¶èœ", "å·èœ-å››å·", "æ»¬èœ-ä¸Šæµ·", "äº¬èœ-å®˜åºœèœ", "äº¬å·æ»¬", "å°ç£èœ", "é †å¾·èœ",
    "æ»‡èœ-é›²å—", "æ¹˜èœ-æ¹–å—", "æ¡‚èœ-å»£è¥¿", "æ±åŒ—èœ", "é²èœ-å±±æ±", "æ–°ç–†èœ", "è¾²å®¶èœ", "é–©èœ-ç¦å»º", "æ±Ÿæµ™èœ",
    "é»”èœ-è²´å·ž", "é™èœ-é™è¥¿", "è’™å¤èœ", "æ™‰èœ-å±±è¥¿", "é„‚èœ-æ¹–åŒ—", "æ·®æšèœ", "æ—¥æœ¬èœ", "éŸ“åœ‹èœ", "æ³°åœ‹èœ",
    "è¶Šå—èœ", "å°å°¼èœ", "æ–°åŠ å¡èœ", "é¦¬ä¾†è¥¿äºžèœ", "è²å¾‹è³“èœ", "ç·¬ç”¸èœ", "å°åº¦èœ", "å°¼æ³Šçˆ¾èœ", "æ–¯é‡Œè˜­å¡èœ",
    "ä¸­æ±èœ", "åœ°ä¸­æµ·èœ", "åœŸè€³å…¶èœ", "é»Žå·´å«©èœ", "æ‘©æ´›å“¥", "åŸƒåŠèœ", "éžæ´²èœ", "çŒ¶å¤ªèœ", "å¸Œè‡˜èœ", "æ„å¤§åˆ©èœ",
    "æ³•åœ‹èœ", "ç¾Žåœ‹èœ", "è‹±åœ‹èœ", "è¥¿ç­ç‰™èœ", "å¾·åœ‹èœ", "æ¯”åˆ©æ™‚èœ", "æ¾³æ´²èœ", "è‘¡åœ‹èœ", "ç‘žå£«èœ", "æ„›çˆ¾è˜­èœ",
    "æ±æ­èœ", "è·è˜­èœ", "å¥§åœ°åˆ©èœ", "è¥¿å¼", "å¢¨è¥¿å“¥èœ", "å¤å·´èœ", "é˜¿æ ¹å»·èœ", "ç§˜é­¯èœ", "å·´è¥¿èœ", "æ¸¯å¼",
    "ä¸­èœ", "ç²µèœ", "äºžæ´²èœ", "è¥¿é¤", "ä¸­æ±-åœ°ä¸­æµ·èœ", "ä¸­å—ç¾Žèœ", "å¤šåœ‹èœ"
]

district_ids = [
    1008, -35243, -35244, -35242, 1001, -9006, 1003, -9007, 1011, 1022, 1019, 1026, 1004, 1023, 1014, 1009, 1018, 1024,
    1013, 1005, 1017, 1025, 1027, 1012, 1020, 1021, 1010, 1002, -9151, 1007, 1015, 1016, 2008, 2010, 2028, 2011, 2029,
    2019, 2026, 2006, 2003, 2022, 2015, 2004, 2013, 2005, 2001, 2016, 2031, 2007, 2009, 2002, 2030, 2027, 2020, 2032,
    2021, 2024, 2025, 2012, 3019, 3018, 3015, 3005, 3003, 3002, 3007, 3012, 3009, 3020, -35260, -35259, -35276, 3004,
    3008, 3001, 3014, 3006, 3013, 3017, 3010, 3022, 3021, 3011, 3016, 4009, 4002, 4004, 4001, 4006, 4005, 4010, 4003, 4011
]
thread_local = threading.local()

def get_driver():
    """Initialize a Chrome WebDriver instance with proper settings."""
    if not hasattr(thread_local, "driver"):
        options = Options()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)')
        thread_local.driver = webdriver.Chrome(options=options)
    return thread_local.driver
```
## ðŸ” Scraping Data from OpenRice
```
def safe_find(element, selector, attribute=None):
    try:
        if isinstance(selector, dict):
            found = element.find(**selector)
        else:
            found = element.find(selector)
        if found and attribute:
            return found.get(attribute)
        elif found:
            return found.text.strip()
    except:
        pass
    return ''

def find_address(info_section):
    divs = info_section.find_all('div')
    for div in divs:
        text = div.text.strip()
        if text and not text.startswith('ä¸Š') and not div.get('class'):
            return text
    return ''

def scrape_openrice(cuisine, district_id, max_retries=2, delay=5):
    encoded_cuisine = quote(cuisine)
    base_url = f'https://www.openrice.com/zh/hongkong/restaurants/cuisine/{encoded_cuisine}?sortBy=ORScoreDesc&districtId={district_id}'
    print(f"Scraping URL: {base_url}")

    for attempt in range(max_retries):
        try:
            driver = get_driver()
            driver.set_page_load_timeout(30)  # Set page load timeout to 30 seconds
            driver.get(base_url)

            scroll_pause_time = 2
            last_height = driver.execute_script("return document.body.scrollHeight")

            while True:
                driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(scroll_pause_time)
                new_height = driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height

            print('Finished scrolling')

            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, "poi-list-cell")))

            soup = BeautifulSoup(driver.page_source, 'html.parser')

            restaurants = soup.find_all('div', class_='poi-list-cell')
            print(f"Found {len(restaurants)} restaurants")

            data = []
            for restaurant in restaurants:
                # Skip closed restaurants
                if restaurant.find('section', class_='pois-closed-restaurants-container'):
                    continue

                info_section = restaurant.find('section', class_='poi-list-cell-desktop-right-top-info-section')
                if info_section:
                    name_element = info_section.find('div', class_='poi-name')
                    if name_element and '(å·²æ¬é·)' in name_element.text:
                        continue  # Skip restaurants that have relocated

                    name = name_element.text.strip() if name_element else ''
                    name = re.sub(r'\s*\(å·²æ¬é·\)\s*', '', name)  # Remove "(å·²æ¬é·)" if present

                    address = find_address(info_section)

                    line_info = info_section.find('div', class_='poi-list-cell-line-info')
                    if line_info:
                        links = line_info.find_all('a', class_='poi-list-cell-line-info-link')
                        district = links[0].text.strip() if len(links) > 0 else ''
                        cuisine = links[1].text.strip() if len(links) > 1 else ''
                        restaurant_type = links[2].text.strip() if len(links) > 2 else ''
                        price_span = line_info.find('span', class_='poi-list-cell-line-info-link')
                        price = price_span.text.strip() if price_span else ''
                    else:
                        district = cuisine = restaurant_type = price = ''

                    # Find RestaurantID
                    link = restaurant.find('a', class_='poi-list-cell-desktop-right-link-overlay')
                    restaurant_id = ''
                    if link:
                        match = re.search(r'r(\d+)', link['href'])
                        if match:
                            restaurant_id = match.group(1)

                    data.append({
                        'Name': name,
                        'District': district,
                        'Address': address,
                        'Cuisine': cuisine,
                        'Type': restaurant_type,
                        'Price': price,
                        'RestaurantID': restaurant_id
                    })

            time.sleep(delay)  # Add delay between requests
            return data

        except (WebDriverException, TimeoutException, NoSuchElementException) as e:
            print(f"Attempt {attempt + 1} failed: {str(e)}")
            if attempt < max_retries - 1:
                print(f"Retrying in {delay} seconds...")
                time.sleep(delay)
            else:
                print("Max retries reached. Moving to next URL.")
                return []
```
## ðŸ’¾ Saving Data to Excel
```
def main():
    all_data = []
    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
        future_to_url = {executor.submit(scrape_openrice, cuisine, district_id): (cuisine, district_id)
                         for cuisine in cuisines for district_id in district_ids}
        for future in concurrent.futures.as_completed(future_to_url):
            cuisine, district_id = future_to_url[future]
            try:
                data = future.result()
                if data:
                    all_data.extend(data)
            except Exception as exc:
                print(f'{cuisine} {district_id} generated an exception: {exc}')

    # Save to Excel
    df = pd.DataFrame(all_data)
    df.to_excel('openrice_restaurants_1.xlsx', index=False, engine='openpyxl')
    print("Data saved to openrice_restaurants_1.xlsx")

    # Close all drivers
    for thread in threading.enumerate():
        if hasattr(thread, "driver"):
            thread.driver.quit()

if __name__ == "__main__":
    main()
```
